import datetime
import hashlib
import os
import re
import pandas as pd
import argparse
from collections import OrderedDict

# Set the display options
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', None)

class PRIAConversionMapGenerator:
    # Constants
    TYPE_COLUMN = 'TYPE'
    TYPE_COLUMN_VALUE_GROUP = 'GROUP'
    SOURCE_COLUMN = 'SOURCE_PATH'
    TARGET_COLUMN = 'TARGET_PATH'
    IS_SELECTED_COLUMN = 'IS_SELECTED'
    DO_NOT_MAP_COLUMN = 'DO NOT MAP'
    DO_GROUP_NEEDS_PREDICATES_COLUMN = 'GROUP_NEEDS_PREDICATES'
    GROUP_IS_SELECTED_COLUMN = 'GROUP_IS_SELECTED'

    def __init__(self, augmented_keystone_report, source, target, run_test=True, log=False, generate_csv=False):
        self.non_ambiguous_keystone_report = augmented_keystone_report
        self.source = source
        self.target = target
        self.run_test = run_test
        self.log = log
        self.generate_csv = generate_csv

        self.log_message("Initializing dataframes...")

        # Create JSON and CSV output file names
        self.json_output_file_name = f'{source.lower().replace(" ", "_")}_to_{target.lower().replace(" ", "_")}_conversion.json'
        self.csv_output_file_name = f'{source.lower().replace(" ", "_")}_to_{target.lower().replace(" ", "_")}_qualified_group.csv'
        os.makedirs('conversion_maps', exist_ok=True)

        # Create the file name with the timestamp
        current_time = datetime.datetime.now()
        timestamp = current_time.strftime("%Y%m%d_%H%M%S")
        self.log_file_name = f"logfile_{timestamp}.log"
        # Create the log directory if it doesn't exist
        os.makedirs('log', exist_ok=True)

        # Load the data. The data is assumed to be in an Excel file with the columns 'SOURCE_PATH', 'TARGET_PATH', 'IS_SELECTED', and 'DO NOT MAP'.
        # You will obtain this file by running the script select_default_conversions_pass1.py on the Keystone report first, and then 
        # select_group_default_conversions_pass3.py that will run on the file generated by the first script.
        self.df = pd.read_excel(augmented_keystone_report)

        # Create a new dataframe that contains only the groups that need predicates. If a group is in this dataframe, it means that the group needs 
        # to use qualifiers.
        self.group_needs_predicate = self.df[(self.df[self.DO_GROUP_NEEDS_PREDICATES_COLUMN] == "YES") & (self.df[self.GROUP_IS_SELECTED_COLUMN] == "YES")]

        # Create working dataframes. This is the data that will be used to generate the conversion maps. It filtered out the rows that are not selected,
        # IS_SELECTED_COLUMN: Indicate the row that is selected among the group of rows that have the same source path.
        # DO_NOT_MAP_CULUMN: Indicate the row that should not be mapped. This occurs when we chose a normalized path instead of a qualified path. 
        #                    In this case, we don't map the qualifier (for instance).
        # GROUP_IS_SELECTED_COLUMN: All groups are marked with this flag. If a group is used among the selected field, it will be marked as selected.
        self.selected_df = self.df[(self.df[self.IS_SELECTED_COLUMN] == "YES") & (self.df[self.DO_NOT_MAP_COLUMN] != 'DO NOT MAP') & (self.df[self.GROUP_IS_SELECTED_COLUMN] == "YES")]
        
        # Just the field now
        self.selected_field_df = self.selected_df[(self.selected_df[self.TYPE_COLUMN] != self.TYPE_COLUMN_VALUE_GROUP)]

        # Because we are simplyfing the xpath (removong predicate when we can), we need to check if the xpath is already processed. We will use a 
        # set to store the processed values.
        self.processed_additional_predicated_node = set()   
        self.other_processed_node = set()   
        
        self.node_not_output = set()

        self.log_message("...")


    def log_message(self, message):
        if self.log:
            with open('log/' + self.log_file_name, 'a') as f:
                print(message, file=f)
        print(message)

    def remove_last_node(self, xpath):
        return '/'.join(xpath.split('/')[:-1])

    # Replace the base path in the full path with the new base path. This is used to build the conversion map.
    def replace_base_path(self, base_path, new_base_path, full_path):
        if full_path.startswith(base_path):
            return new_base_path + '/' + full_path[len(base_path):].lstrip('/')
        return full_path

    # Remove a given number of nodes (levels_up) at the end of the xpath. 
    def extract_ancestor_xpath(self, path, levels_up):
        # Initialize the position to the end of the string
        position = len(path)
    
        # Loop to find the position of the "/" character for the specified number of levels
        for _ in range(levels_up):
            position = path.rfind('/', 0, position)
            if position == -1:
            # If there are not enough levels, return an empty string
                return ''

        # Return the substring up to the position found
        return path[:position]

    # Simplify the xpath by removing the predicate.
    # Example: 
    # Input: PackingSlip/Header/References[ReferenceQual='BL']/ReferenceID
    # Output: PackingSlip/Header/References/ReferenceID
    def remove_predicate(self, xpath):
        # Regular expression to remove the predicate
        return re.sub(r'\[.*?\]', '', xpath)

    def output_json(self, line):
        with open('conversion_maps/' + self.json_output_file_name, 'a') as f:
            print(line, file=f)

    def output_csv(self, line):
        with open('conversion_maps/' + self.csv_output_file_name, 'a') as f:
            print(line, file=f)

    # Is an ancestor if the compared_xpath is the parent of the predicate node if there is one, 
    # or the last node if there is no predicate.
    def is_ancestor(self, compared_xpath, column_xpath):
        # Check if the column XPath contains a predicate
        contains_predicate = '[' in column_xpath
        
        # Determine the number of nodes to compare
        if contains_predicate:
            # Exclude the last two nodes if there is a predicate
            column_xpath = self.extract_ancestor_xpath(column_xpath, 2)
        else:
            # Exclude the last node if there is no predicate
            column_xpath = self.extract_ancestor_xpath(column_xpath, 1)
        
        # Check if the compared XPath is an ancestor
        return column_xpath.startswith(compared_xpath)

    # Test if this predicate xpath having a predicate on the node before the last node need to keep the predicate.
    # It need to keep the predicate if the compared_xpath is equal column xpath once we remove the preeicate and the last
    # node.
    def predicate_is_needed(self, compared_xpath, column_xpath):
        if '[' not in column_xpath:
            return False
        
        # Check if the column XPath contains a predicate
        column_xpath = self.remove_predicate(column_xpath)
        column_xpath = self.extract_ancestor_xpath(column_xpath, 1)
        
        # Check if the compared XPath is an ancestor
        return column_xpath == compared_xpath

    # Process a group of rows with the same source path (if more than one row = ambiguity).
    # Output the conversion map for the group to the Json file.
    def process_group(self, data):
        count = 1
        source_value = data[self.SOURCE_COLUMN].iloc[0]
        type_value = data[self.TYPE_COLUMN].iloc[0]
        self.node_not_output.add(source_value)
        
        # First check if the group is a group or a field. 
        if type_value != self.TYPE_COLUMN_VALUE_GROUP: # IT IS A FIELD
            is_predicate_can_be_removed = False
            
            # First:
            # Validate if this source belongs to group that has and needs predicate. This information is given 
            # by appplying the "is_predicate" function on the "group_needs_predicate" dataframe's source column. 
            # "group_needs_predicate" is a filter on the augmented Keystone report wich say for each group if it 
            # needs predicate or not. 
            # 
            # If the The source already contains a predicate, we need to know if we can simplify the xpath 
            # by removing the predicate. Thus, if the group to which the source belongs is not present in 
            # "group_needs_predicate", we will simplify the xpath by removing the predicate of the source and the 
            # target.
            need_predicate_boolean_mask1 = self.group_needs_predicate[self.SOURCE_COLUMN].apply(lambda x: self.predicate_is_needed(x, source_value))

            # Filter the DataFrame using the boolean mask
            applicable_predicated_group_df1 = self.group_needs_predicate[need_predicate_boolean_mask1]
            
            # If the result is empty, it means that the predicate is not needed               
            is_predicate_can_be_removed = len(applicable_predicated_group_df1) == 0 and '[' in source_value
                
            # This block is just to return the function if predicate is not needed but the resulted simplified xpath
            # has already been processed.
            if '[' in source_value:
                if is_predicate_can_be_removed: # No predicate needed
                    # Simplify the xpath by removing the predicate
                    reformat_source_path = self.remove_predicate(source_value)
                
                    # Because we are simplifying the xpath, we need to check if the xpath is already processed. 
                    # We will use a set to store the processed values.
                    if reformat_source_path in self.processed_additional_predicated_node: 
                        self.node_not_output.discard(source_value)
                        return None
                    else:
                        self.processed_additional_predicated_node.add(reformat_source_path)
                
            #
            # Even if we can remove the predicate, we still need to validate if additional predicate is needed.
            # ==> We will continue the process to validate if additional predicate is needed.
            #
            
            # Second:
            # Do the source (containing or not a predicate) need additional predicate? This information is given 
            # by appplying the "is_ancestor" function on the "group_needs_predicate" dataframe's source column. 
            # If the source has at least one ancestor that needs a predicate, we will then need to add the 
            # predicates to the appropriate group of the source and the target. 
            # 
            # NB1: Theoricaly, the source can have more than one group that needs predicate, thus the first "for" 
            # loop. But the code is written to handle only one group needing predicate.
            # 
            # NB2: The "is_ancestor" function will not look for the first group starting on the right if the source
            # already contains a predicate. That is because this would be a case of simplification of the xpath
            # (which is done in the previous block above).
            need_predicate_boolean_mask2 = self.group_needs_predicate[self.SOURCE_COLUMN].apply(lambda x: self.is_ancestor(x, source_value))
            
            if need_predicate_boolean_mask2.any():
                # Filter the DataFrame using the Boolean Series
                applicable_predicated_group_df2 = self.group_needs_predicate[need_predicate_boolean_mask2]
                
                # Loop all groups that need predicate (according to exsiting use cases, it should be only one group)
                for index1, row_needs_additional_predicate in applicable_predicated_group_df2.iterrows():
                    source_ancestor_needing_predicate = row_needs_additional_predicate[self.SOURCE_COLUMN]
                    target_ancestor_needing_predicate = row_needs_additional_predicate[self.TARGET_COLUMN]
                    
                    # Loop all target of the group. Since ambiguity is resolved, there should be only one target.
                    for index, target_value in data[self.TARGET_COLUMN].items():
                        # Make sure the "row_needs_additional_predicate" Serie does apply to the current group.
                        if not target_value.startswith(target_ancestor_needing_predicate):
                            continue
                        
                        # Handle the case where the source already contains a predicate but needs to be simplified as well
                        # as it needs additional predicate for an ancestor group.
                        working_source_value = source_value
                        working_target_value = target_value

                        if is_predicate_can_be_removed:
                            working_source_value = self.remove_predicate(source_value)
                            working_target_value = self.remove_predicate(target_value)
                    
                        # Filter the selected_field_df DataFrame to only include the children of the predicated group
                        temp_df_predicated_groups = self.selected_field_df[
                            (self.selected_field_df[self.SOURCE_COLUMN].str.startswith(source_ancestor_needing_predicate + '[')) & 
                            (self.selected_field_df[self.TARGET_COLUMN].str.startswith(target_ancestor_needing_predicate + '['))
                        ]        
                        
                        if len(temp_df_predicated_groups) == 0:
                            print(f"Not suppose to happen!!!!!!!!!!! =====> temp_df_predicated_groups is empty. source_ancestor_needing_predicate: {source_ancestor_needing_predicate}, target_ancestor_needing_predicate: {target_ancestor_needing_predicate}")
                                
                        # Create a new dataframe that contains only the predicated groups. If a group is in this dataframe, 
                        # it means that the group needs to use qualifiers.
                        df_predicated_groups = temp_df_predicated_groups.copy()
                        # Remove the last node from the source path (teh field), so he last node wil become the predicate.
                        df_predicated_groups[self.SOURCE_COLUMN] = df_predicated_groups[self.SOURCE_COLUMN].apply(self.remove_last_node)
                        # Because we removed the fields, we created duplicates. Let's remove them.
                        df_predicated_groups = df_predicated_groups.drop_duplicates(subset=[self.SOURCE_COLUMN])

                        # Loop all predicated path of the group
                        # List of predicated nodes
                        for index2, predicated_row in df_predicated_groups.iterrows():
                            predicated_source_field = predicated_row[self.SOURCE_COLUMN]
                            predicated_target_field = self.extract_ancestor_xpath(predicated_row[self.TARGET_COLUMN], 1)

                            if '[' not in predicated_target_field: # That should not happen
                                print(f"Not suppose to happen!!!!!!!!!!! =====> predicated_target_field: {predicated_target_field}")
                            
                            # Remove the base path from the source and target fields and replace it with the predicated one.
                            modified_source_path = self.replace_base_path(source_ancestor_needing_predicate, predicated_source_field, working_source_value)
                            modified_target_path = self.replace_base_path(target_ancestor_needing_predicate, predicated_target_field, working_target_value)
                            
                            self.output_json(f'\t"{modified_source_path}": [')
                            self.output_json(f'\t\t"{modified_target_path}"')
                            self.output_json('\t],')
                            
                            self.node_not_output.discard(source_value)

            # The source does not need additional predicate
            else:
                
                if is_predicate_can_be_removed:
                    # If no group needing predicate is found, simplify the xpath by removing the predicate if needed.
                    if '[' in source_value:

                        reformat_source_path = self.remove_predicate(source_value)
                        if reformat_source_path not in self.other_processed_node:
                            self.output_json(f'\t"{reformat_source_path}": [')

                            for index, target_value in data[self.TARGET_COLUMN].items():
                                target_value = self.remove_predicate(target_value)
                                self.output_json(f'\t\t"{target_value}"')

                            # Add the new_path to the set of processed values
                            self.output_json(f'\t],')

                            self.other_processed_node.add(reformat_source_path)
                        self.node_not_output.discard(source_value)
                else:

                    if source_value not in self.other_processed_node:

                        self.output_json(f'\t"{source_value}": [')

                        for index, target_value in data[self.TARGET_COLUMN].items():
                            self.output_json(f'\t\t"{target_value}"')
                        self.output_json('\t],')

                        self.other_processed_node.add(source_value)
                        self.node_not_output.discard(source_value)

        # type_value == TYPE_COLUMN_VALUE_GROUP. We will output the conversion instructions for the group
        else:  
            self.output_json(f'\t"{source_value}": [')
  
            for index, target_value in data[self.TARGET_COLUMN].items():
 
                if count == len(data):
                    self.output_json(f'\t\t"{target_value}"')
                else:
                    self.output_json(f'\t\t"{target_value}",')
                count += 1

            self.output_json(f'\t],')
            self.node_not_output.discard(source_value)
            
    # Main function to generate the conversion maps    
    def generate_conversion_maps(self):
        self.log_message("Processing...")

        # Write the first bracket
        self.output_json("{")
        self.selected_df.groupby(self.SOURCE_COLUMN, group_keys=False).apply(self.process_group)

        # Replace the last line in the JSON file with a closing bracket without a comma
        # Assuming json_output_file_name is defined and contains the path to the file
        with open('conversion_maps/' + self.json_output_file_name, 'r') as file:
            lines = file.readlines()

        # Replace the last line
        lines[-1] = '\t]\n'

        # Write all lines back to the file
        with open('conversion_maps/' + self.json_output_file_name, 'w') as file:
            file.writelines(lines)

        # Close the first bracket
        self.output_json("}")

        # Output list of qualified groups if requested
        if self.generate_csv:
            self.output_csv("QUALIFIED_GROUPS")
            self.processed_additional_predicated_node.clear()
            for index, row in self.group_needs_predicate.iterrows():
                if row[self.SOURCE_COLUMN] not in self.processed_additional_predicated_node:
                    self.output_csv(row[self.SOURCE_COLUMN])
                    self.processed_additional_predicated_node.add(row[self.SOURCE_COLUMN])

            self.log_message(f"Processing complete. Results saved to 'conversion_maps/{self.json_output_file_name}'.")
        
        if len(self.node_not_output) > 0:
            for node in self.node_not_output:
                print(f"Node not output: {node}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate PRIA conversion maps for a non-ambiguous conversion. Need to run select_default_conversions.py prior to this script.')
    parser.add_argument('non_ambiguous_keystone_report', type=str, help='Keystone report on which you previously run select_default_conversions.py.')
    parser.add_argument('source', type=str, help='Name and version of the canonical source, e.g., ShippingLabel 3.0')
    parser.add_argument('target', type=str, help='Name and version of the canonical target, e.g., Shipment 7.7')
    parser.add_argument('--run_test', action='store_true', default=True, help='Run the test at the end (default: False)')
    parser.add_argument('--log', action='store_true', default=False, help='Will log in log subdirectory. (default: False)')
    parser.add_argument('--generate_csv', action='store_true', default=False, help='Will generate a CSV file that contains all the group that will need to use qualifiers. (default: False)')
    args = parser.parse_args()

    generator = PRIAConversionMapGenerator(args.non_ambiguous_keystone_report, args.source, args.target, args.run_test, args.log, args.generate_csv)
    generator.generate_conversion_maps()